{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['comp.sys.mac.hardware',  'misc.forsale', 'talk.politics.mideast', 'rec.autos']\n",
    "#categories = ['alt.atheism','rec.sport.baseball', 'comp.graphics', 'sci.space', 'rec.sport.hockey',  'comp.windows.x']\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=2, stop_words='english', use_idf=True, ngram_range=(1, 2))\n",
    "#vectorizer = TfidfVectorizer(min_df=5, max_df=0.5, stop_words='english')\n",
    "matrix = vectorizer.fit_transform(dataset.data)\n",
    "dense_matrix = matrix.todense()\n",
    "Idata=dense_matrix\n",
    "Idata= np.squeeze(np.asarray(Idata))\n",
    "print \"data shape : \", dense_matrix.shape\n",
    "labels = dataset.target\n",
    "actual_label=labels.tolist()\n",
    "print labels, type(labels),len(labels)\n",
    "true_k = np.unique(labels).shape[0]\n",
    "print (true_k)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.cluster.supervised import adjusted_rand_score\n",
    "from sklearn.preprocessing import normalize\n",
    "#from plotter import plot_max_Sil_Score\n",
    "import sompy\n",
    "from Evolutionary.Evoution1 import Mapsize, generate_matingPool,Generate,mapping,calculate_dunn_index\n",
    "from nsga2.main import Select\n",
    "from normalization import NormalizatorFactory\n",
    "from sklearn import cluster\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from  pbm_Index import cal_pbm_index\n",
    "Iflag=0       #Flag\n",
    "x = []        #store all center\n",
    "DunnIndex=[]  #List of ari against each solution in population\n",
    "sil_sco=[]    #List of Silhoutte Score angainst each solution in population\n",
    "counter=0\n",
    "chromosome = int(input(\"Enter the no. of Chromosomes:\"))\n",
    "n = 4    #int(input(\"Enter the no. of clusters to be formed:\"))\n",
    "Initial_Label=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,chromosome+1):\n",
    "    counter+=1\n",
    "    pop = []\n",
    "    cluster = KMeans(n_clusters=n, init='k-means++', max_iter=100, n_init=1)\n",
    "    cluster.fit(Idata)\n",
    "    label = cluster.predict(Idata)\n",
    "    centers = cluster.cluster_centers_\n",
    "    a=centers.tolist()\n",
    "    for j in range(len(a)):\n",
    "        for k in range(len(Idata[0])):\n",
    "            pop.append(a[j][k])\n",
    "    x.insert(i,pop)\n",
    "    ss=silhouette_score(Idata, label)\n",
    "    pbm= cal_pbm_index(n,Idata,centers,label)\n",
    "    #di = calculate_dunn_index(Idata,label,n_cluster=n)\n",
    "    sil_sco.insert(i, ss)\n",
    "    DunnIndex.insert(i,pbm)\n",
    "    Initial_Label.append(label.tolist())\n",
    "Final_label= list(Initial_Label)\n",
    "\n",
    "\n",
    "\n",
    "population=np.array(x)\n",
    "print \"Population   : \", population\n",
    "print \"PBM    : \",DunnIndex\n",
    "print \"Sil score   : \",sil_sco\n",
    "H= 5 #int(input(\"Enter the size of neighbourhood mating pool: \")) #Size of mating pool\n",
    "\n",
    "data=population\n",
    "final_label=[]\n",
    "print \"############################################\"\n",
    "\n",
    "\n",
    "TDS = [0]\n",
    "generation=0\n",
    "generationmax=10\n",
    "self_pop = None\n",
    "zdt_definitions = None\n",
    "plotter = None\n",
    "problem = None\n",
    "evolution = None\n",
    "while generation<generationmax:\n",
    "\n",
    "\n",
    "    #TDS = []\n",
    "    print \"-------------------------------------------------------------------------------\"\n",
    "    print \"Generation No., data size , shape  : \",generation, data.size, population.shape\n",
    "    if not data.size==0:\n",
    "        for i in range(len(population)):\n",
    "            MatingPool = np.arange(len(population))  # Generate mating pool\n",
    "            MatingPool=np.delete(MatingPool,i)\n",
    "            print \"Mating Pool : \", MatingPool,type(MatingPool)  # Mating pool of each neuron\n",
    "            nsol=Generate(MatingPool,  H, population,i,CR=0.8,F=0.8,mutation_prob=0.6,eta=20) #Generate new solution/Children\n",
    "            print \"Nsol generated using mating Pool : \", nsol\n",
    "            print \"Idata shape : \", Idata.shape[1]\n",
    "            cluster_center = np.reshape(nsol, (-1, Idata.shape[1]))\n",
    "            #break\n",
    "            print \"new solutiojn in cluster center form : \", cluster_center\n",
    "            cluster = KMeans(n_clusters=n, init=cluster_center, max_iter=100, n_init=1)\n",
    "            cluster.fit(Idata)\n",
    "            new_center=cluster.cluster_centers_\n",
    "            label = cluster.predict(Idata)\n",
    "            new_ss = silhouette_score(Idata, label)\n",
    "            print \"new solution after clustering  : \", new_center\n",
    "            new_pbm = cal_pbm_index(n, Idata, new_center, label)\n",
    "            a = new_center.tolist()\n",
    "            new_sol=[]\n",
    "            for j in range(len(a)):\n",
    "                for k in range(len(Idata[0])):\n",
    "                    new_sol.append(a[j][k])\n",
    "            print \"new solution in list form : \", new_sol\n",
    "            #new_di = calculate_dunn_index(Idata,label,n_cluster=n)\n",
    "            print \"NewSS\",new_ss\n",
    "            print \"NewDI\", new_pbm\n",
    "            U_population,objectives,self_population,zdt_definitions,plotter,problem,evolution,Final_label = Select(population,sil_sco,DunnIndex,new_sol,new_pbm,new_ss,generation,self_pop,zdt_definitions,plotter,problem,evolution,Final_label,label)\n",
    "            self_pop=self_population\n",
    "            population= np.array(U_population)\n",
    "\n",
    "        generation+=1\n",
    "    data = np.array(population)\n",
    "    \"\"\"\n",
    "    num_rows, num_cols = data.shape\n",
    "    b = data.tolist()\n",
    "    for j in range(num_rows):\n",
    "\n",
    "\n",
    "\n",
    "        a=population[j].tolist()\n",
    "        if a not in b:\n",
    "            TDS.insert(j,a)\n",
    "    print \"TDS\",np.array(TDS)\n",
    "    data = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "    data=np.array(TDS)\n",
    "    generation+=1\n",
    "    Iflag = generation\n",
    "    \"\"\"\n",
    "\n",
    "    #break\n",
    "print \"Returned pop\",population\n",
    "print \"Objectives  (Sil. score, PBM)  : \",objectives\n",
    "return_ss = [item[0] for item in objectives]\n",
    "return_di = [item[1] for item in objectives]\n",
    "print \"PBM : \",return_di\n",
    "print \"SS  : \",return_ss\n",
    "\n",
    "\n",
    "\n",
    "print \"Max. PBM : \",max(return_di)\n",
    "print \"Max SS  : \",max(return_ss)\n",
    "#x= return_ss.index(max(return_ss))\n",
    "y= return_di.index(max(return_di))\n",
    "#updated_label1=list(Initial_Label[x])\n",
    "updated_label2=list(Initial_Label[y])\n",
    "#print \"ARI1\",adjusted_rand_score(actual_label,updated_label1)\n",
    "\n",
    "print \"ARI2\",adjusted_rand_score(actual_label,updated_label2)\n",
    "all_ari=[]\n",
    "for i in range (len(return_ss)):\n",
    "    x=adjusted_rand_score(actual_label, Final_label[i])\n",
    "    print \"ARI \",i, x\n",
    "    all_ari.insert(i,x)\n",
    "print \"Max ARI\",max(all_ari)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# agglomerative= cluster.AgglomerativeClustering(n_clusters=n, linkage='average', affinity='euclidean')\n",
    "# idx= agglomerative.fit_predict(Idata)\n",
    "# hlabels=agglomerative.labels_\n",
    "# ss = silhouette_score(Idata, hlabels)\n",
    "# print \"Silhoutte Score Agglomerative-\",ss\n",
    "# di = calculate_dunn_index(Idata,hlabels,n_cluster=n)\n",
    "# print \"Dunn Index Agglomerative\",di\n",
    "# vms= v_measure_score(actual_label, hlabels)\n",
    "# print \"V- measure score, Agglomerative\",vms\n",
    "# nmi=normalized_mutual_info_score(actual_label,hlabels)\n",
    "# print \"NMIS Agglomerative{}\".format(nmi)\n",
    "# new_ari = adjusted_rand_score(actual_label, hlabels)\n",
    "# print \"Agglomerative ARI=\",new_ari\n",
    "# print \"\\n\\n\"\n",
    "#\n",
    "# agglomerative= cluster.AgglomerativeClustering(n_clusters=n, linkage='ward', affinity='euclidean')\n",
    "# idx= agglomerative.fit_predict(Idata)\n",
    "# hlabels=agglomerative.labels_\n",
    "# ss = silhouette_score(Idata, hlabels)\n",
    "# print \"Silhoutte Score Agglomerative-\",ss\n",
    "# di = calculate_dunn_index(Idata,hlabels,n_cluster=n)\n",
    "# print \"Dunn Index Agglomerative\",di\n",
    "# vms= v_measure_score(actual_label, hlabels)\n",
    "# print \"V- measure score, Agglomerative\",vms\n",
    "# nmi=normalized_mutual_info_score(actual_label,hlabels)\n",
    "# print \"NMIS Agglomerative{}\".format(nmi)\n",
    "# new_ari = adjusted_rand_score(actual_label, hlabels)\n",
    "# print \"Agglomerative ARI=\",new_ari\n",
    "# print \"\\n\\n\"\n",
    "#\n",
    "# agglomerative= cluster.AgglomerativeClustering(n_clusters=n, linkage='complete', affinity='euclidean')\n",
    "# idx= agglomerative.fit_predict(Idata)\n",
    "# hlabels=agglomerative.labels_\n",
    "# ss = silhouette_score(Idata, hlabels)\n",
    "# print \"Silhoutte Score Agglomerative-\",ss\n",
    "# di = calculate_dunn_index(Idata,hlabels,n_cluster=n)\n",
    "# print \"Dunn Index Agglomerative\",di\n",
    "# vms= v_measure_score(actual_label, hlabels)\n",
    "# print \"V- measure score, Agglomerative\",vms\n",
    "# nmi=normalized_mutual_info_score(actual_label,hlabels)\n",
    "# print \"NMIS Agglomerative{}\".format(nmi)\n",
    "# new_ari = adjusted_rand_score(actual_label, hlabels)\n",
    "# print \"Agglomerative ARI=\",new_ari\n",
    "# print \"\\n\\n\"\n",
    "#\n",
    "print \"------------------------------------\"\n",
    "cluster = KMeans(n_clusters=n)\n",
    "cluster.fit(Idata)\n",
    "label = cluster.predict(Idata)\n",
    "#print label\n",
    "ss = silhouette_score(Idata, label)\n",
    "print \"Silhoutte Score K-means-\",ss\n",
    "di = calculate_dunn_index(Idata,label,n_cluster=n)\n",
    "print \"Dunn Index K-means\",di\n",
    "#vms= v_measure_score(actual_label, label)\n",
    "#print \"V- measure score, K-means\",vms\n",
    "# nmi=normalized_mutual_info_score(actual_label,label)\n",
    "# print \"NMIS Kmeans{}\".format(nmi)\n",
    "new_ari = adjusted_rand_score(actual_label, label)\n",
    "print \"ARI=\",new_ari\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
